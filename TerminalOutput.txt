Using username "hadoop".
Authenticating with public key "imported-openssh-key"
Passphrase for key "imported-openssh-key":

       __|  __|_  )
       _|  (     /   Amazon Linux AMI
      ___|\___|___|

https://aws.amazon.com/amazon-linux-ami/2018.03-release-notes/
7 package(s) needed for security, out of 13 available
Run "sudo yum update" to apply all updates.

EEEEEEEEEEEEEEEEEEEE MMMMMMMM           MMMMMMMM RRRRRRRRRRRRRRR
E::::::::::::::::::E M:::::::M         M:::::::M R::::::::::::::R
EE:::::EEEEEEEEE:::E M::::::::M       M::::::::M R:::::RRRRRR:::::R
  E::::E       EEEEE M:::::::::M     M:::::::::M RR::::R      R::::R
  E::::E             M::::::M:::M   M:::M::::::M   R:::R      R::::R
  E:::::EEEEEEEEEE   M:::::M M:::M M:::M M:::::M   R:::RRRRRR:::::R
  E::::::::::::::E   M:::::M  M:::M:::M  M:::::M   R:::::::::::RR
  E:::::EEEEEEEEEE   M:::::M   M:::::M   M:::::M   R:::RRRRRR::::R
  E::::E             M:::::M    M:::M    M:::::M   R:::R      R::::R
  E::::E       EEEEE M:::::M     MMM     M:::::M   R:::R      R::::R
EE:::::EEEEEEEE::::E M:::::M             M:::::M   R:::R      R::::R
E::::::::::::::::::E M:::::M             M:::::M RR::::R      R::::R
EEEEEEEEEEEEEEEEEEEE MMMMMMM             MMMMMMM RRRRRRR      RRRRRR

[hadoop@ip-172-20-5-227 ~]$ ls
(reverse-i-search)`s': l^C
[hadoop@ip-172-20-5-227 ~]$ aws s3 cp s3://801101040bucket/CC_Shashank/SparkSQLScala_a.jar .
download: s3://801101040bucket/CC_Shashank/SparkSQLScala_a.jar to ./SparkSQLScala_a.jar
[hadoop@ip-172-20-5-227 ~]$ spark-submit --class org.SparkSQL.Driver ./SparkSQLScala.jar s3://801101040bucket/CC_Shashank/data.txt s3://801101040bucket/CC_Shashank/SparkSQLOutput
^C[hadoop@ip-172-20-5-227 ~]$ spark-submit --class org.SparkSQL.Driver ./SparkSQLScala_a.jar s3://801101040bucket/CC_Shashank/data.txt s3://801101040bucket/CC_Shashank/SparkSQLOutput
20/02/1213:24:31 INFO SparkContext: Running Spark version 2.4.3
20/02/1213:24:31 INFO SparkContext: Submitted application: SparkAction
20/02/1213:24:31 INFO SecurityManager: Changing view acls to: hadoop
20/02/1213:24:31 INFO SecurityManager: Changing modify acls to: hadoop
20/02/1213:24:31 INFO SecurityManager: Changing view acls groups to:
20/02/1213:24:31 INFO SecurityManager: Changing modify acls groups to:
20/02/1213:24:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
20/02/1213:24:31 INFO Utils: Successfully started service 'sparkDriver' on port 34509.
20/02/1213:24:31 INFO SparkEnv: Registering MapOutputTracker
20/02/1213:24:32 INFO SparkEnv: Registering BlockManagerMaster
20/02/1213:24:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/02/1213:24:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/02/1213:24:32 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-9f3494d9-65ef-4688-ae91-41f4415fbbeb
20/02/1213:24:32 INFO MemoryStore: MemoryStore started with capacity 424.4 MB
20/02/1213:24:32 INFO SparkEnv: Registering OutputCommitCoordinator
20/02/1213:24:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/02/1213:24:32 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ip-172-20-5-227.us-east-2.compute.internal:4040
20/02/1213:24:32 INFO SparkContext: Added JAR file:/home/hadoop/./SparkSQLScala_a.jar at spark://ip-172-20-5-227.us-east-2.compute.internal:34509/jars/SparkSQLScala_a.jar with timestamp 1569162272555
20/02/1213:24:32 INFO Executor: Starting executor ID driver on host localhost
20/02/1213:24:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38757.
20/02/1213:24:32 INFO NettyBlockTransferService: Server created on ip-172-20-5-227.us-east-2.compute.internal:38757
20/02/1213:24:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/02/1213:24:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-20-5-227.us-east-2.compute.internal, 38757, None)
20/02/1213:24:32 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-20-5-227.us-east-2.compute.internal:38757 with 424.4 MB RAM, BlockManagerId(driver, ip-172-20-5-227.us-east-2.compute.internal, 38757, None)
20/02/1213:24:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-20-5-227.us-east-2.compute.internal, 38757, None)
20/02/1213:24:32 INFO BlockManager: external shuffle service port = 7337
20/02/1213:24:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-20-5-227.us-east-2.compute.internal, 38757, None)
20/02/1213:24:34 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/local-1569162272667
20/02/1213:24:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 237.8 KB, free 424.2 MB)
20/02/1213:24:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.2 KB, free 424.2 MB)
20/02/1213:24:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-20-5-227.us-east-2.compute.internal:38757 (size: 24.2 KB, free: 424.4 MB)
20/02/1213:24:35 INFO SparkContext: Created broadcast 0 from textFile at Driver.scala:19
20/02/1213:24:41 INFO SharedState: loading hive config file: file:/etc/spark/conf.dist/hive-site.xml
20/02/1213:24:41 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('hdfs:///user/spark/warehouse').
20/02/1213:24:41 INFO SharedState: Warehouse path is 'hdfs:///user/spark/warehouse'.
20/02/1213:24:44 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/02/1213:24:50 INFO CodeGenerator: Code generated in 712.164228 ms
20/02/1213:24:55 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
20/02/1213:24:55 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.DirectFileOutputCommitter
20/02/1213:24:55 INFO DirectFileOutputCommitter: Nothing to setup since the outputs are written directly.
20/02/1213:24:55 INFO GPLNativeCodeLoader: Loaded native gpl library
20/02/1213:24:55 INFO LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 7e6c862e89bc8db32c064454a55af74ddff73bae]
20/02/1213:24:55 INFO FileInputFormat: Total input files to process : 1
20/02/1213:24:55 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78
20/02/1213:24:55 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
20/02/1213:24:55 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at SparkHadoopWriter.scala:78)
20/02/1213:24:55 INFO DAGScheduler: Parents of final stage: List()
20/02/1213:24:55 INFO DAGScheduler: Missing parents: List()
20/02/1213:24:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at saveAsTextFile at Driver.scala:31), which has no missing parents
20/02/1213:24:56 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 95.4 KB, free 424.1 MB)
20/02/1213:24:56 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 34.7 KB, free 424.0 MB)
20/02/1213:24:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-20-5-227.us-east-2.compute.internal:38757 (size: 34.7 KB, free: 424.4 MB)
20/02/1213:24:56 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1201
20/02/1213:24:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at saveAsTextFile at Driver.scala:31) (first 15 tasks are for partitions Vector(0))
20/02/1213:24:56 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
20/02/1213:24:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7883 bytes)
20/02/1213:24:56 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
20/02/1213:24:56 INFO Executor: Fetching spark://ip-172-20-5-227.us-east-2.compute.internal:34509/jars/SparkSQLScala_a.jar with timestamp 1569162272555
20/02/1213:24:56 INFO TransportClientFactory: Successfully created connection to ip-172-20-5-227.us-east-2.compute.internal/172.31.20.153:34509 after 142 ms (0 ms spent in bootstraps)
20/02/1213:24:56 INFO Utils: Fetching spark://ip-172-20-5-227.us-east-2.compute.internal:34509/jars/SparkSQLScala_a.jar to /mnt/tmp/spark-a7189342-1aa7-656c-9d8a-61bc97a91a47/userFiles-323cdc64-c232-45a1-a00b-40eae9f5b804/fetchFileTemp5865826835746669918.tmp
20/02/1213:24:57 INFO Executor: Adding file:/mnt/tmp/spark-a7189342-1aa7-656c-9d8a-61bc97a91a47/userFiles-323cdc64-c232-45a1-a00b-40eae9f5b804/SparkSQLScala_a.jar to class loader
20/02/1213:24:57 INFO HadoopRDD: Input split: s3://801101040bucket/CC_Shashank/data.txt:0+53593
20/02/1213:24:57 INFO S3NativeFileSystem: Opening 's3://801101040bucket/CC_Shashank/data.txt' for reading
20/02/1213:24:58 INFO CodeGenerator: Code generated in 70.452249 ms
20/02/1213:24:58 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.DirectFileOutputCommitter
20/02/1213:24:58 INFO MultipartUploadOutputStream: close closed:false s3://801101040bucket/CC_Shashank/SparkSQLOutput/part-00000
20/02/1213:24:58 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20190922142453_0008_m_000000_0
20/02/1213:24:58 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1619 bytes result sent to driver
20/02/1213:24:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2239 ms on localhost (executor driver) (1/1)
20/02/1213:24:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
20/02/1213:24:58 INFO DAGScheduler: ResultStage 0 (runJob at SparkHadoopWriter.scala:78) finished in 2.748 s
20/02/1213:24:58 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 3.034806 s
20/02/1213:24:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
20/02/1213:24:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
20/02/1213:24:58 INFO DirectFileOutputCommitter: Direct Write: ENABLED
20/02/1213:24:58 INFO DirectFileOutputCommitter: Nothing to clean up since no temporary files were written.
20/02/1213:24:58 INFO MultipartUploadOutputStream: close closed:false s3://801101040bucket/CC_Shashank/SparkSQLOutput/_SUCCESS
20/02/1213:24:58 INFO SparkHadoopWriter: Job job_20190922142453_0008 committed.
20/02/1213:24:58 INFO SparkContext: Invoking stop() from shutdown hook
20/02/1213:24:58 INFO SparkUI: Stopped Spark web UI at http://ip-172-20-5-227.us-east-2.compute.internal:4040
20/02/1213:24:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/02/1213:24:59 INFO MemoryStore: MemoryStore cleared
20/02/1213:24:59 INFO BlockManager: BlockManager stopped
20/02/1213:24:59 INFO BlockManagerMaster: BlockManagerMaster stopped
20/02/1213:24:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/02/1213:24:59 INFO SparkContext: Successfully stopped SparkContext
20/02/1213:24:59 INFO ShutdownHookManager: Shutdown hook called
20/02/1213:24:59 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-a7189342-1aa7-656c-9d8a-61bc97a91a47
20/02/1213:24:59 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-7443a82e-9d0a-318d-af47-a16ee9c7542b
[hadoop@ip-172-20-5-227 ~]$
